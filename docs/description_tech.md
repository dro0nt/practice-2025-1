# Описание технологии нейросети

## Введение
Этот документ предоставляет подробное описание технологии, использованной для создания простой нейросети с двумя скрытыми слоями, реализованной на Python с использованием numpy. В документе рассмотрены следующие аспекты:

* **Исследование предметной области** — основы нейросетей, выбор активационных функций.
* **Описание технологии** — структура сети, активации, обучение.
* **Архитектура** — слои, веса, функции активации.
* **Процессы технологии** — обучение с обратным распространением ошибки.
* **Модификация проекта** — использование нормализации входных данных, батчевого обучения.

---

## Часть 1: Исследование предметной области и последовательность действий по созданию технологии

### 1.1 Исследование предметной области
Перед созданием нейросети было проведено исследование, чтобы понять, как работают многослойные перцептроны, какие функции активации применять, и какие методы обучения использовать. Основные этапы:

1. **Изучение основ нейросетей:**
   * Многослойный перцептрон (MLP) с несколькими слоями.
   * Задача бинарной классификации.
2. **Выбор функций активации:**
   * ReLU — эффективна для скрытых слоев.
   * tanh — нелинейная, центрированная около нуля.
   * sigmoid — для выходного слоя, для вероятности.
3. **Метод обучения:**
   * Обратное распространение ошибки (backpropagation).
   * Градиентный спуск с использованием батчей.
4. **Нормализация данных:**
   * Использование StandardScaler для стандартизации входов.
5. **Инициализация весов:**
   * Случайные веса в диапазоне [-1, 1].

### 1.2 Последовательность действий по созданию технологии
1. Подготовка данных и нормализация.
2. Инициализация весов для всех слоев.
3. Реализация функций активации и их производных.
4. Реализация прямого прохода (forward pass).
5. Реализация обратного распространения (backpropagation).
6. Обучение нейросети в цикле по эпохам с батчевой обработкой.
7. Отслеживание ошибки обучения и вывод результатов.

### 1.3 Требования к технологии
* Эффективность обучения с использованием батчей.
* Использование нелинейных функций активации для повышения выразительности модели.
* Визуализация динамики ошибки.
* Возможность менять активации и количество нейронов.

---

## Часть 2 - Описание технологии

**Нейросеть** — это многослойный перцептрон с архитектурой 3-4-4-1 (3 входа, два скрытых слоя по 4 нейрона, 1 выход).

### Функции активации

- **ReLU** для первого скрытого слоя:
  ```
  def relu(x):
      return np.maximum(0, x)
  ```

- tanh во втором скрытом слое для улучшения сигнала и симметричного отклика.
- Сигмоида на выходе для вероятностного вывода.

### Обратное распространение

- Вычисление дельт ошибки для каждого слоя с учётом производных функций активации.
- Корректировка весов с использованием градиентного спуска без отдельного параметра скорости обучения (можно добавить).

### Обработка данных

- Нормализация входов стандартизацией с помощью sklearn StandardScaler для повышения эффективности обучения.

### Обучение

- Обучение в течение 10,000 эпох с батчами по 2 элемента.
- Внутри каждой эпохи данные перемешиваются для устойчивости обучения.

### Визуализация

- График динамики средней абсолютной ошибки по эпохам с шагом 100, позволяющий контролировать процесс сходимости.

---

## Часть 3 — Архитектура системы

### 3.1 Компоненты нейросети

- **Входной слой:**  
  Принимает стандартизированные входные данные размерности (N, 3), где N — число примеров.

- **Скрытые слои:**  
  Первый скрытый слой: матричное умножение с весами (3,4), ReLU активация.  
  Второй скрытый слой: умножение с весами (4,4), tanh активация.

- **Выходной слой:**  
  Умножение с весами (4,1), функция активации — сигмоида.

- **Обратное распространение:**  
  Вычисление ошибок и градиентов для всех слоёв, корректировка весов.

### 3.2 Взаимодействие компонентов

- Входные данные → Нормализация → Скрытый слой 1 → Скрытый слой 2 → Выход
- Ошибка выходного слоя → Обратное распространение → Обновление весов
- Повторение цикла по эпохам

---

## Часть 4 — Процессы технологии

### 4.1 Этапы разработки

- Создание и нормализация данных: подготовка простого тренировочного набора и стандартизация.
- Определение функций активации и их производных: подготовка к обучению и обратному распространению.
- Инициализация весов: случайное заполнение весовых матриц.
- Реализация цикла обучения: батчевый градиентный спуск с перемешиванием данных.
- Отслеживание ошибки: сбор и визуализация метрики ошибки для мониторинга обучения.
- Вывод результатов: проверка работы модели на тренировочном наборе.

### 4.2 Процессы эксплуатации

- Запуск скрипта Python с кодом нейросети.
- Мониторинг графика ошибки для оценки качества обучения.
- Получение прогнозов сети для входных данных.

---

## Часть 5 — Модификация проекта

Для улучшения и расширения проекта можно:

- Ввести параметр скорости обучения (learning rate) для более контролируемого обновления весов.
- Добавить функцию потерь (например, MSE или BCE) вместо простой абсолютной ошибки.
- Использовать более сложные архитектуры (больше слоёв или нейронов).
- Реализовать регуляризацию для борьбы с переобучением.
- Добавить сохранение и загрузку модели.
- Перенести код на фреймворки TensorFlow или PyTorch для масштабируемости.
